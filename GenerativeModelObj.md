## [How (not) to Train your Generative Model: Scheduled Sampling,  Likelihood, Adversary?](https://arxiv.org/abs/1511.05101)

### Intro

How should we choose our objective function for generative model. Scheduled sampling has good empirical performance. GAN has even better performance, theoretical basis?

### Symptom

At generation time, it can generate a prefix that is never seen in the training data. And since proceeding prefix is conditioned on previous one, it is hard to recover from the mistake.

### Fail of Scheduled sampling

1. Scheduled sampling: $$\epsilonâ€‹$$ from a Bernoulli distribution decide whether we keep the original symbol or use one generated by the model given previous symbols.
2. Dss fails to meet minimization at Q = P, which means lack of consistency.

### Rethink symptom/ diagnosis

Mismatch between loss function used for training and for evaluating.

### An ideal model

1. Ideally, the evaluation loss is $$\min{-\mathbb{E}_{x~Q}\log{P(x)}}$$
2. However, this would be maximized by a model that deterministically picks the most likely stimulus. We want to enforce diversity by maximizing the entropy of Q.
3. $$\min{-\mathbb{E}_{x~Q}\log{P(x)}+\mathbb{E}_{x~Q}\log{Q(x)}}$$
4. But this is not practical since it is only well defined if P is positive and bounded in the full support of Q. Which is not the case when P is empirical and Q is a smooth prob model.

### Rethink symptom

1. A balance between $$KL[Q][P]$$ and $$KL[P][Q]$$ 
2. Minimizing $$KL[P][Q]$$ corresponds to moment matching and has a tendency to find models Q that cover all the models of P. Minimizing $$KL[Q][P]$$ has a mode-seeking behavior. Q will typically concentrate around the largest mode of P

### GAN obj

1. A generalized JS divergence is a mixture of the above two KLD.
2. $$JS_\pi[P][Q] = \pi KL[P][\pi P + (1-\pi )Q]+(1-\pi)KL[Q][\pi P + (1-\pi)Q]$$

### Conclusion

1. ML should not be used for generative tasks.
2. Scheduled sampling theoretically not good.
3. An ideal obj proposed
4. GJSD proposed
5. GAN worth a try.
